{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Sales\n",
       "0  230.1   22.1\n",
       "1   44.5   10.4\n",
       "2   17.2    9.3\n",
       "3  151.5   18.5\n",
       "4  180.8   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv = pd.DataFrame({\n",
    "    \"TV\": [230.1, 44.5, 17.2, 151.5, 180.8],\n",
    "    \"Sales\":[22.1, 10.4, 9.3, 18.5, 12.9]\n",
    "}\n",
    ")\n",
    "\n",
    "adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAleklEQVR4nO3df3DU9Z3H8dcmIWtCshuj5BfZ0FgtlKK5ynHICQpDGsh5CKa06nBeROYYvaAFPKzxDvrDu0nP63XQK5W7azVyPW0rRxC4lisV8sNr4Go09Uc1FS/CCglQkd38gBCTz/0R2N5KAknc7Hc/yfMxszPu9/vZzTvukDxn9/v9xmWMMQIAALBUnNMDAAAAfBLEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsluD0ACOtt7dXR48eVWpqqlwul9PjAACAQTDGqK2tTTk5OYqLu/h7L6M+Zo4ePSqfz+f0GAAAYBj8fr9yc3MvumbUx0xqaqqkvv8ZHo/H4WkAAMBgBINB+Xy+0O/xixn1MXP+oyWPx0PMAABgmcEcIsIBwAAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqo/4KwAAAYAT09kgn6qTTLVJStjRhjhQX78goxAwAABga/zap4StS5/u/35acK01/XPKVRH0cPmYCAACD598m1S0NDxlJ6jzSt92/LeojETMAAGBwenv63pGR6WfnuW0Nq/vWRZGjMVNRUaEZM2YoNTVVGRkZWrJkiZqamkL7T548qfvvv1+TJ09WUlKS8vLy9MADDygQCDg4NQAAY9SJugvfkQljpE5/37oocjRmampqVFZWpv3792vPnj3q7u5WUVGROjo6JElHjx7V0aNH9e1vf1tvvPGGKisrtXv3bq1YscLJsQEAGJtOt0R2XYS4jDH9vVfkiBMnTigjI0M1NTW66aab+l3z/PPP68/+7M/U0dGhhIRLH78cDAbl9XoVCATk8XgiPTIAAGPHsWrpxXmXXjd/n5Q59xN9qaH8/o6ps5nOf3yUnp5+0TUej2fAkOnq6lJXV1fofjAYjOyQAACMVRPm9J211HlE/R834+rbP2FOVMeKmQOAe3t7tXr1at14442aNm1av2t+97vf6dFHH9XKlSsHfJ6Kigp5vd7QzefzjdTIAACMLXHxfadfS5JcH9t57v70jVG/3kzMfMx033336Wc/+5leeukl5ebmXrA/GAzqC1/4gtLT07Vjxw6NGzeu3+fp750Zn8/Hx0wAAERKv9eZ8fWFTISuM2Pdx0yrVq3Srl27VFtb22/ItLW1aeHChUpNTVVVVdWAISNJbrdbbrd7JMcFAGBs85VIExdzBWBJMsbo/vvvV1VVlaqrq5Wfn3/BmmAwqAULFsjtdmvHjh267LLLHJgUAACEiYv/xAf5RoqjMVNWVqZnn31WL7zwglJTU9Xa2ipJ8nq9SkpKUjAYVFFRkTo7O/XDH/5QwWAwdEDvhAkTFB/vTAECAIDY4egxMy7Xxw8e6vP000/r7rvvVnV1tebN6/8UsObmZn3qU5+65Nfg1GwAAOxjzTEzl+qouXPnXnINAAAY22Lm1GwAAIDhIGYAAIDViBkAAGA1YgYAAFiNmAEAAFYjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFiNmAEAAFYjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFiNmAEAAFYjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFiNmAEAAFZzNGYqKio0Y8YMpaamKiMjQ0uWLFFTU1PYmjNnzqisrExXXHGFUlJS9MUvflHHjh1zaGIAABBrHI2ZmpoalZWVaf/+/dqzZ4+6u7tVVFSkjo6O0Jo1a9Zo586dev7551VTU6OjR4+qpKTEwakBAEAscRljjNNDnHfixAllZGSopqZGN910kwKBgCZMmKBnn31WS5culSS9/fbb+uxnP6v6+nrdcMMNl3zOYDAor9erQCAgj8cz0t8CAACIgKH8/o6pY2YCgYAkKT09XZLU0NCg7u5uFRYWhtZMmTJFeXl5qq+v7/c5urq6FAwGw24AAGD0ipmY6e3t1erVq3XjjTdq2rRpkqTW1lYlJiYqLS0tbG1mZqZaW1v7fZ6Kigp5vd7QzefzjfToAADAQTETM2VlZXrjjTf0ox/96BM9T3l5uQKBQOjm9/sjNCEAAIhFCU4PIEmrVq3Srl27VFtbq9zc3ND2rKwsnT17VqdOnQp7d+bYsWPKysrq97ncbrfcbvdIjwwAAGKEo+/MGGO0atUqVVVVae/evcrPzw/bP336dI0bN04vvvhiaFtTU5MOHz6sWbNmRXtcAAAQgxx9Z6asrEzPPvusXnjhBaWmpoaOg/F6vUpKSpLX69WKFSu0du1apaeny+Px6P7779esWbMGdSYTAAAY/Rw9NdvlcvW7/emnn9bdd98tqe+ieQ8++KCee+45dXV1acGCBfre97434MdMH8ep2QAA2Gcov79j6jozI4GYAQDAPtZeZwYAAGCoiBkAAGA1YgYAAFiNmAEAAFYjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFiNmAEAAFYjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFgtwekBAACX0NsjnaiTTrdISdnShDlSXLzTUwExg5gBgFjm3yY1fEXqfP/325JzpemPS74S5+YCYggfMwFArPJvk+qWhoeMJHUe6dvu3+bMXECMIWYAIBb19vS9IyPTz85z2xpW960DxjhiBgBi0Ym6C9+RCWOkTn/fOmCMI2YAIBadbonsOmAUI2YAIBYlZUd2HTCKETMAEIsmzOk7a0muARa4pGRf3zpgjCNmACAWxcX3nX4t6cKgOXd/+kauNwOImAGA2OUrkeZslZInhm9Pzu3bznVmAElcNA8AYpuvRJq4mCsAAxdBzABArIuLlzLnOj0FELP4mAkAAFiNmAEAAFYjZgAAgNUcjZna2lotWrRIOTk5crlc2r59e9j+9vZ2rVq1Srm5uUpKStLUqVO1efNmZ4YFAAAxydGY6ejoUEFBgTZt2tTv/rVr12r37t364Q9/qLfeekurV6/WqlWrtGPHjihPCgAAYpWjZzMVFxeruLh4wP2//OUvVVpaqrlz50qSVq5cqX/+53/W//zP/+jWW2+N0pQAACCWxfQxM3/8x3+sHTt26MiRIzLGaN++ffrtb3+roqKiAR/T1dWlYDAYdgMAAKNXTMfMP/3TP2nq1KnKzc1VYmKiFi5cqE2bNummm24a8DEVFRXyer2hm8/ni+LEAAAg2mI+Zvbv368dO3aooaFB//iP/6iysjL94he/GPAx5eXlCgQCoZvf74/ixAAAINpi9grAp0+f1iOPPKKqqirdcsstkqTrrrtOjY2N+va3v63CwsJ+H+d2u+V2u6M5KgAAcFDMvjPT3d2t7u5uxcWFjxgfH6/e3l6HpgIAALHG0Xdm2tvbdfDgwdD95uZmNTY2Kj09XXl5ebr55pu1bt06JSUladKkSaqpqdGWLVv0ne98x8GpAQBALHEZY4xTX7y6ulrz5s27YHtpaakqKyvV2tqq8vJy/fznP9fJkyc1adIkrVy5UmvWrJHL5RrU1wgGg/J6vQoEAvJ4PJH+FgAAwAgYyu9vR2MmGogZAADsM5Tf3zF7zAwAAMBgEDMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrRSRmenp61NjYqA8//DASTwcAADBow4qZ1atX6wc/+IGkvpC5+eabdf3118vn86m6ujqS8wEAAFzUsGJm69atKigokCTt3LlTzc3Nevvtt7VmzRr99V//dUQHBAAAuJhhxczvfvc7ZWVlSZJ++tOf6ktf+pI+85nP6J577tHrr78e0QEBAAAuZlgxk5mZqd/85jfq6enR7t279YUvfEGS1NnZqfj4+EE/T21trRYtWqScnBy5XC5t3779gjVvvfWWbr31Vnm9Xo0fP14zZszQ4cOHhzM2AAAYhYYVM8uXL9eXv/xlTZs2TS6XS4WFhZKkAwcOaMqUKYN+no6ODhUUFGjTpk397n/33Xc1e/ZsTZkyRdXV1Xrttde0fv16XXbZZcMZGwAAjEIuY4wZzgO3bt0qv9+vL33pS8rNzZUkPfPMM0pLS9PixYuHPojLpaqqKi1ZsiS07Y477tC4ceP0b//2b8MZUZIUDAbl9XoVCATk8XiG/TwAACB6hvL7O2G4X2Tp0qWSpDNnzoS2lZaWDvfpLtDb26v//M//1EMPPaQFCxbo1VdfVX5+vsrLy8OC5+O6urrU1dUVuh8MBiM2EwAAiD3D+pipp6dHjz76qCZOnKiUlBT97//+ryRp/fr1oVO2P6njx4+rvb1d3/rWt7Rw4UL9/Oc/12233aaSkhLV1NQM+LiKigp5vd7QzefzRWQeAAAQm4YVM3/3d3+nyspKPfbYY0pMTAxtnzZtmr7//e9HZLDe3l5J0uLFi7VmzRr9wR/8gR5++GH96Z/+qTZv3jzg48rLyxUIBEI3v98fkXkAAEBsGlbMbNmyRf/yL/+iZcuWhZ29VFBQoLfffjsig1155ZVKSEjQ1KlTw7Z/9rOfvejZTG63Wx6PJ+wGAABGr2HFzJEjR3T11VdfsL23t1fd3d2feChJSkxM1IwZM9TU1BS2/be//a0mTZoUka8BAADsN6wDgKdOnaq6uroLomLr1q36/Oc/P+jnaW9v18GDB0P3m5ub1djYqPT0dOXl5WndunW6/fbbddNNN2nevHnavXu3du7cyZ9MAAAAIcOKmQ0bNqi0tFRHjhxRb2+vtm3bpqamJm3ZskW7du0a9PO8/PLLmjdvXuj+2rVrJfWdFVVZWanbbrtNmzdvVkVFhR544AFNnjxZ//Ef/6HZs2cPZ2wAADAKDfs6M3V1dfrmN7+pX//612pvb9f111+vDRs2qKioKNIzfiJcZwYAAPsM5ff3sGPGFsQMAAD2Gcrv72EdAAwAABArBn3MzOWXXy6XyzWotSdPnhz2QAAAAEMx6JjZuHHjCI4BAAAwPIOOmUj+3SUAAIBIGfYfmjzvzJkzOnv2bNg2DrQFAADRMqwDgDs6OrRq1SplZGRo/Pjxuvzyy8NuAAAA0TKsmHnooYe0d+9ePfnkk3K73fr+97+vb3zjG8rJydGWLVsiPSMAAMCAhvUx086dO7VlyxbNnTtXy5cv15w5c3T11Vdr0qRJ+vd//3ctW7Ys0nMCAAD0a1jvzJw8eVJXXXWVpL7jY86fij179mzV1tZGbjoAAIBLGFbMXHXVVWpubpYkTZkyRT/5yU8k9b1jk5aWFrHhAAAALmVYMbN8+XL9+te/liQ9/PDD2rRpky677DKtWbNG69ati+iAAAAAFxORv8106NAhNTQ06Oqrr9Z1110Xibkihr/NBACAfUbsbzPV19dr165dYdvOHwh877336rvf/a66urqGPjEAAMAwDSlmvvnNb+rNN98M3X/99de1YsUKFRYWqry8XDt37lRFRUXEhwQAABjIkGKmsbFR8+fPD93/0Y9+pJkzZ+pf//VftWbNGj3xxBOhg4EBAACiYUgx8+GHHyozMzN0v6amRsXFxaH7M2bMkN/vj9x0AAAAlzCkmMnMzAydkn327Fm98soruuGGG0L729raNG7cuMhOCAAAcBFDipk/+ZM/0cMPP6y6ujqVl5crOTlZc+bMCe1/7bXX9OlPfzriQwIAAAxkSH/O4NFHH1VJSYluvvlmpaSk6JlnnlFiYmJo/1NPPaWioqKIDwkAADCQYV1nJhAIKCUlRfHx8WHbT548qZSUlLDAcRrXmQEAwD5D+f09rD806fV6+92enp4+nKcDAAAYtmH9OQMAAIBYQcwAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsJqjMVNbW6tFixYpJydHLpdL27dvH3DtvffeK5fLpY0bN0ZtPgAAEPscjZmOjg4VFBRo06ZNF11XVVWl/fv3KycnJ0qTAQAAWyQ4+cWLi4tVXFx80TVHjhzR/fffr//6r//SLbfcEqXJAACALRyNmUvp7e3VXXfdpXXr1ulzn/vcoB7T1dWlrq6u0P1gMDhS4wEAgBgQ0wcA//3f/70SEhL0wAMPDPoxFRUV8nq9oZvP5xvBCQEAgNNiNmYaGhr0+OOPq7KyUi6Xa9CPKy8vVyAQCN38fv8ITgkAAJwWszFTV1en48ePKy8vTwkJCUpISNChQ4f04IMP6lOf+tSAj3O73fJ4PGE3AAAwesXsMTN33XWXCgsLw7YtWLBAd911l5YvX+7QVAAAINY4GjPt7e06ePBg6H5zc7MaGxuVnp6uvLw8XXHFFWHrx40bp6ysLE2ePDnaowIAgBjlaMy8/PLLmjdvXuj+2rVrJUmlpaWqrKx0aCoAAGATR2Nm7ty5MsYMev177703csMAAAArxewBwAAAAINBzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsFqC0wMAANCv3h7pRJ10ukVKypYmzJHi4p2eCjGImAEAxB7/NqnhK1Ln+7/flpwrTX9c8pU4NxdiEh8zAQBii3+bVLc0PGQkqfNI33b/NmfmQswiZgAAsaO3p+8dGZl+dp7b1rC6bx1wDjEDAIgdJ+oufEcmjJE6/X3rgHOIGQBA7DjdEtl1GBOIGQBA7EjKjuw6jAnEDAAgdkyY03fWklwDLHBJyb6+dcA5xAwAIHbExfedfi3pwqA5d3/6Rq43gzDEDAAgtvhKpDlbpeSJ4duTc/u2c50ZfAwXzQMAxB5fiTRxMVcAxqAQMwCA2BQXL2XOdXoKWICPmQAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAAGA1YgYAAFiNmAEAAFYjZgAAgNUcjZna2lotWrRIOTk5crlc2r59e2hfd3e3vvrVr+raa6/V+PHjlZOToz//8z/X0aNHnRsYAADEHEdjpqOjQwUFBdq0adMF+zo7O/XKK69o/fr1euWVV7Rt2zY1NTXp1ltvdWBSAAAQq1zGGOP0EJLkcrlUVVWlJUuWDLjmV7/6lf7oj/5Ihw4dUl5e3qCeNxgMyuv1KhAIyOPxRGhaAAAwkoby+zshSjNFRCAQkMvlUlpa2oBrurq61NXVFbofDAajMBkAAHCKNQcAnzlzRl/96ld15513XrTQKioq5PV6QzefzxfFKQEAQLRZETPd3d368pe/LGOMnnzyyYuuLS8vVyAQCN38fn+UpgQAAE6I+Y+ZzofMoUOHtHfv3kt+buZ2u+V2u6M0HQAAcFpMx8z5kHnnnXe0b98+XXHFFU6PBAAAYoyjMdPe3q6DBw+G7jc3N6uxsVHp6enKzs7W0qVL9corr2jXrl3q6elRa2urJCk9PV2JiYlOjQ0AAGKIo6dmV1dXa968eRdsLy0t1de//nXl5+f3+7h9+/Zp7ty5g/oanJoNAIB9rDk1e+7cubpYS8XIJXAAAEAMs+JsJgAAgIEQMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAaglOD4AY1tsjnaiTTrdISdnShDlSXLzTUwEAEIaYQf/826SGr0id7/9+W3KuNP1xyVfi3FwAAHwMHzPhQv5tUt3S8JCRpM4jfdv925yZCwCAfhAzCNfb0/eOjEw/O89ta1jdtw4AgBhAzCDciboL35EJY6ROf986AABiADGDcKdbIrsOAIARRswgXFJ2ZNcBADDCiBmEmzCn76wluQZY4JKSfX3rAACIAcQMwsXF951+LenCoDl3f/pGrjcDAIgZxAwu5CuR5myVkieGb0/O7dvOdWYAADGEi+ahf74SaeJirgAMAIh5xAwGFhcvZc51egoAAC7K0Y+ZamtrtWjRIuXk5Mjlcmn79u1h+40x2rBhg7Kzs5WUlKTCwkK98847zgwLAABikqMx09HRoYKCAm3atKnf/Y899pieeOIJbd68WQcOHND48eO1YMECnTlzJsqTAgCAWOXox0zFxcUqLi7ud58xRhs3btTf/M3faPHixZKkLVu2KDMzU9u3b9cdd9wRzVEBAECMitmzmZqbm9Xa2qrCwsLQNq/Xq5kzZ6q+vn7Ax3V1dSkYDIbdAADA6BWzMdPa2ipJyszMDNuemZkZ2tefiooKeb3e0M3n843onAAAwFkxGzPDVV5erkAgELr5/X6nRwIAACMoZmMmKytLknTs2LGw7ceOHQvt64/b7ZbH4wm7AQCA0StmYyY/P19ZWVl68cUXQ9uCwaAOHDigWbNmOTgZAACIJY6ezdTe3q6DBw+G7jc3N6uxsVHp6enKy8vT6tWr9bd/+7e65pprlJ+fr/Xr1ysnJ0dLlixxbmgAABBTHI2Zl19+WfPmzQvdX7t2rSSptLRUlZWVeuihh9TR0aGVK1fq1KlTmj17tnbv3q3LLrvMqZEBAECMcRljjNNDjKRgMCiv16tAIMDxMwAAWGIov79j9pgZAACAwSBmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDVHL3OjNV6e6QTddLpFikpW5owR4qLd3oqAADGHGJmOPzbpIavSJ3v/35bcq40/XHJV+LcXAAAjEF8zDRU/m1S3dLwkJGkziN92/3bnJkLAIAxipgZit6evndk1N9Fk89ta1jdtw4AAEQFMTMUJ+oufEcmjJE6/X3rAABAVBAzQ3G6JbLrAADAJ0bMDEVSdmTXAQCAT4yYGYoJc/rOWpJrgAUuKdnXtw4AAEQFMTMUcfF9p19LujBozt2fvpHrzQAAEEXEzFD5SqQ5W6XkieHbk3P7tnOdGQAAooqL5g2Hr0SauJgrAAMAEAOImeGKi5cy5zo9BQAAYx4fMwEAAKsRMwAAwGrEDAAAsBoxAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrjforABtjJEnBYNDhSQAAwGCd/719/vf4xYz6mGlra5Mk+Xw+hycBAABD1dbWJq/Xe9E1LjOY5LFYb2+vjh49qtTUVLlcLqfHGfWCwaB8Pp/8fr88Ho/T42AAvE524HWyA6/TyDDGqK2tTTk5OYqLu/hRMaP+nZm4uDjl5uY6PcaY4/F4+EdtAV4nO/A62YHXKfIu9Y7MeRwADAAArEbMAAAAqxEziCi3262vfe1rcrvdTo+Ci+B1sgOvkx14nZw36g8ABgAAoxvvzAAAAKsRMwAAwGrEDAAAsBoxAwAArEbMYMi+/vWvy+Vyhd2mTJkS2n/mzBmVlZXpiiuuUEpKir74xS/q2LFjDk48NtTW1mrRokXKycmRy+XS9u3bw/YbY7RhwwZlZ2crKSlJhYWFeuedd8LWnDx5UsuWLZPH41FaWppWrFih9vb2KH4Xo9+lXqe77777gn9fCxcuDFvD6zTyKioqNGPGDKWmpiojI0NLlixRU1NT2JrB/Kw7fPiwbrnlFiUnJysjI0Pr1q3TRx99FM1vZUwgZjAsn/vc59TS0hK6vfTSS6F9a9as0c6dO/X888+rpqZGR48eVUlJiYPTjg0dHR0qKCjQpk2b+t3/2GOP6YknntDmzZt14MABjR8/XgsWLNCZM2dCa5YtW6Y333xTe/bs0a5du1RbW6uVK1dG61sYEy71OknSwoULw/59Pffcc2H7eZ1GXk1NjcrKyrR//37t2bNH3d3dKioqUkdHR2jNpX7W9fT06JZbbtHZs2f1y1/+Us8884wqKyu1YcMGJ76l0c0AQ/S1r33NFBQU9Lvv1KlTZty4ceb5558PbXvrrbeMJFNfXx+lCSHJVFVVhe739vaarKws8w//8A+hbadOnTJut9s899xzxhhjfvOb3xhJ5le/+lVozc9+9jPjcrnMkSNHojb7WPLx18kYY0pLS83ixYsHfAyvkzOOHz9uJJmamhpjzOB+1v30pz81cXFxprW1NbTmySefNB6Px3R1dUX3GxjleGcGw/LOO+8oJydHV111lZYtW6bDhw9LkhoaGtTd3a3CwsLQ2ilTpigvL0/19fVOjTvmNTc3q7W1Nex18Xq9mjlzZuh1qa+vV1pamv7wD/8wtKawsFBxcXE6cOBA1Gcey6qrq5WRkaHJkyfrvvvu0wcffBDax+vkjEAgIElKT0+XNLifdfX19br22muVmZkZWrNgwQIFg0G9+eabUZx+9Bv1f2gSkTdz5kxVVlZq8uTJamlp0Te+8Q3NmTNHb7zxhlpbW5WYmKi0tLSwx2RmZqq1tdWZgRH6f///f6iev39+X2trqzIyMsL2JyQkKD09ndcuihYuXKiSkhLl5+fr3Xff1SOPPKLi4mLV19crPj6e18kBvb29Wr16tW688UZNmzZNkgb1s661tbXff3Pn9yFyiBkMWXFxcei/r7vuOs2cOVOTJk3ST37yEyUlJTk4GWC/O+64I/Tf1157ra677jp9+tOfVnV1tebPn+/gZGNXWVmZ3njjjbBjAxFb+JgJn1haWpo+85nP6ODBg8rKytLZs2d16tSpsDXHjh1TVlaWMwMi9P/+42da/P/XJSsrS8ePHw/b/9FHH+nkyZO8dg666qqrdOWVV+rgwYOSeJ2ibdWqVdq1a5f27dun3Nzc0PbB/KzLysrq99/c+X2IHGIGn1h7e7veffddZWdna/r06Ro3bpxefPHF0P6mpiYdPnxYs2bNcnDKsS0/P19ZWVlhr0swGNSBAwdCr8usWbN06tQpNTQ0hNbs3btXvb29mjlzZtRnRp/3339fH3zwgbKzsyXxOkWLMUarVq1SVVWV9u7dq/z8/LD9g/lZN2vWLL3++uth8blnzx55PB5NnTo1Ot/IWOH0Eciwz4MPPmiqq6tNc3Oz+e///m9TWFhorrzySnP8+HFjjDH33nuvycvLM3v37jUvv/yymTVrlpk1a5bDU49+bW1t5tVXXzWvvvqqkWS+853vmFdffdUcOnTIGGPMt771LZOWlmZeeOEF89prr5nFixeb/Px8c/r06dBzLFy40Hz+8583Bw4cMC+99JK55pprzJ133unUtzQqXex1amtrM3/1V39l6uvrTXNzs/nFL35hrr/+enPNNdeYM2fOhJ6D12nk3Xfffcbr9Zrq6mrT0tISunV2dobWXOpn3UcffWSmTZtmioqKTGNjo9m9e7eZMGGCKS8vd+JbGtWIGQzZ7bffbrKzs01iYqKZOHGiuf32283BgwdD+0+fPm3+8i//0lx++eUmOTnZ3HbbbaalpcXBiceGffv2GUkX3EpLS40xfadnr1+/3mRmZhq3223mz59vmpqawp7jgw8+MHfeeadJSUkxHo/HLF++3LS1tTnw3YxeF3udOjs7TVFRkZkwYYIZN26cmTRpkvmLv/iLsFN7jeF1iob+XiNJ5umnnw6tGczPuvfee88UFxebpKQkc+WVV5oHH3zQdHd3R/m7Gf1cxhgT7XeDAAAAIoVjZgAAgNWIGQAAYDViBgAAWI2YAQAAViNmAACA1YgZAABgNWIGAABYjZgBAABWI2YAAIDViBkAMcvlcl30tmjRIrlcLu3fv7/fx8+fP18lJSVRnhpAtCU4PQAADKSlpSX03z/+8Y+1YcMGNTU1hbalpKRo9uzZeuqpp3TDDTeEPfa9997Tvn37tHPnzqjNC8AZvDMDIGZlZWWFbl6vVy6XK2xbSkqKVqxYoR//+Mfq7OwMe2xlZaWys7O1cOFCh6YHEC3EDACrLVu2TF1dXdq6dWtomzFGzzzzjO6++27Fx8c7OB2AaCBmAFgtPT1dt912m5566qnQtn379um9997T8uXLHZwMQLQQMwCsd88996i2tlbvvvuuJOmpp57SzTffrKuvvtrhyQBEAzEDwHrz589XXl6eKisrFQwGtW3bNq1YscLpsQBECWczAbBeXFycli9frh/84AeaOHGiEhMTtXTpUqfHAhAlvDMDYFRYvny5jhw5okceeUR33nmnkpKSnB4JQJQQMwBGhby8PBUWFurDDz/UPffc4/Q4AKLIZYwxTg8BAAAwXLwzAwAArEbMAAAAqxEzAADAasQMAACwGjEDAACsRswAAACrETMAAMBqxAwAALAaMQMAAKxGzAAAAKsRMwAAwGr/B/TaBat5rBbLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(\n",
    "    x = adv['TV'],\n",
    "    y = adv[\"Sales\"],\n",
    "    color = \"orange\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"TV\")\n",
    "plt.ylabel(\"Sales\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(adv['TV'])\n",
    "Y = np.array(adv['Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar encontrar os coeficientes de regressão linear $m$ e $b$, por meio da minimização da diferença entre os valores originais de $y^{(i)}$ e os valores previstos $\\hat{y}^{(i)}$ com a **função de custo (loss Function)** $L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$ para cada exemplo de treino. \n",
    "\n",
    "Para comparar o resultado das previsões $\\hat{Y}$ com o vetor $Y$ de valores originais $y^{(i)}$, você pode pegar a média dos valores da função de custo para cada exemplo:\n",
    "\n",
    "$$E\\left(m, b\\right) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \n",
    "\\frac{1}{2n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)^2,\\tag{1}$$\n",
    "\n",
    "Essa função é conhecida como soma dos quadrados **cost function**. Para o usar o algoritmo de gradient descent, calcule as derivadas parciais de m e b:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E }{ \\partial m } &= \n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)x^{(i)},\\\\\n",
    "\\frac{\\partial E }{ \\partial b } &= \n",
    "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right),\n",
    "\\tag{2}\\end{align}\n",
    "\n",
    "e atualizado os parâmetros iterativamente, usando essas expressões\n",
    "\n",
    "\\begin{align}\n",
    "m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n",
    "b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n",
    "\\tag{3}\\end{align}\n",
    "\n",
    "onde  $\\alpha$ é a taxa de aprendizado (learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Os arrays originais X e Y têm unidades diferentes. Para tornar o algoritmo de descida de gradiente eficiente, é necessário trazê-los para as mesmas unidades. Uma abordagem comum para isso é chamada de normalização: subtrair o valor médio do array de cada um dos elementos no array e dividi-los pelo desvio padrão (uma medida estatística da quantidade de dispersão de um conjunto de valores).\n",
    "\n",
    "A normalização não é obrigatória - o algoritmo de descida de gradiente funcionaria sem ela. No entanto, devido às diferentes unidades de X e Y, a função de custo será muito mais íngreme. Nesse caso, seria necessário usar uma taxa de aprendizado $\\alpha$ significativamente menor, e o algoritmo exigiria milhares de iterações para convergir em vez de algumas dezenas. A normalização ajuda a aumentar a eficiência do algoritmo de descida de gradiente.\n",
    "\n",
    "A normalização é implementada no seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X - np.mean(X))/np.std(X)\n",
    "Y_norm = (Y - np.mean(Y))/np.std(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funçaõ de custo\n",
    "def E(m, b, X, Y):\n",
    "    return 1/(2*len(Y))*np.sum((m*X + b - Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dEdm(m, b, X, Y):\n",
    "    \"\"\"\n",
    "    Calcula a derivada parcial da função de erro em relação à inclinação (m) em uma regressão linear.\n",
    "\n",
    "    Parâmetros:\n",
    "    - m (float): Inclinação atual da reta de regressão.\n",
    "    - b (float): Intercepto atual da reta de regressão.\n",
    "    - X (array-like): Array contendo os valores da variável independente.\n",
    "    - Y (array-like): Array contendo os valores da variável dependente.\n",
    "\n",
    "    Retorna:\n",
    "    - float: Derivada parcial da função de erro em relação à inclinação (m).\n",
    "    \"\"\"\n",
    "    res = 1/len(Y) * np.dot((m*X + b - Y), X)\n",
    "    return res\n",
    "\n",
    "def dEdb(m, b, X, Y):\n",
    "    \"\"\"\n",
    "    Calcula a derivada parcial da função de erro em relação ao intercepto (b) em uma regressão linear.\n",
    "\n",
    "    Parâmetros:\n",
    "    - m (float): Inclinação atual da reta de regressão.\n",
    "    - b (float): Intercepto atual da reta de regressão.\n",
    "    - X (array-like): Array contendo os valores da variável independente.\n",
    "    - Y (array-like): Array contendo os valores da variável dependente.\n",
    "\n",
    "    Retorna:\n",
    "    - float: Derivada parcial da função de erro em relação ao intercepto (b).\n",
    "    \"\"\"\n",
    "    res = 1/len(Y) * np.sum(m*X + b - Y)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8576318592110517\n",
      "2.2204460492503132e-17\n",
      "0.14236814078894797\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(dEdm(0, 0, X_norm, Y_norm))\n",
    "print(dEdb(0, 0, X_norm, Y_norm))\n",
    "print(dEdm(1, 5, X_norm, Y_norm))\n",
    "print(dEdb(1, 5, X_norm, Y_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos implementar a descida do gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(dEdm, dEdb, m, b, X, Y, learning_rate = 0.001, num_iterations = 1000, print_cost=False):\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        m_new = m - learning_rate * dEdm(m, b, X, Y)\n",
    "        b_new = b - learning_rate * dEdb(m, b, X, Y)\n",
    "        \n",
    "        m = m_new\n",
    "        b = b_new\n",
    "        if print_cost:\n",
    "            print (f\"Cost after iteration {iteration}: {E(m, b, X, Y)}\")\n",
    "        \n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5422845484413322, -4.3298697960382076e-19)\n",
      "(0.9863870537929061, 4.521910375044022)\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(dEdm, dEdb, 0, 0, X_norm, Y_norm))\n",
    "print(gradient_descent(dEdm, dEdb, 1, 5, X_norm, Y_norm, learning_rate = 0.01, num_iterations = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar iniciar a descida com m e b iniciando em 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.14694444515177355\n",
      "Cost after iteration 1: 0.13282222295784452\n",
      "Cost after iteration 2: 0.13225733407008733\n",
      "Cost after iteration 3: 0.13223473851457704\n",
      "Cost after iteration 4: 0.1322338346923566\n",
      "Gradient descent result: m_min, b_min = 0.8579063014059991, 2.6645352591003756e-17\n"
     ]
    }
   ],
   "source": [
    "m_initial = 0; b_initial = 0; num_iterations = 30; learning_rate = 1.2\n",
    "m_gd, b_gd = gradient_descent(dEdm, dEdb, m_initial, b_initial, \n",
    "                              X_norm, Y_norm, learning_rate, num_iterations, print_cost=True)\n",
    "\n",
    "print(f\"Gradient descent result: m_min, b_min = {m_gd}, {b_gd}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se de que os conjuntos de dados iniciais foram normalizados. Para fazer as previsões, você precisa normalizar o array `X_pred`, calcular `Y_pred` com os coeficientes da regressão linear `m_gd` e `b_gd` e, em seguida, **desnormalizar** o resultado (realizar o processo inverso de normalização):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV marketing expenses:\n",
      "[ 50 120 175]\n",
      "Predictions of sales using Gradient Descent:\n",
      "[10.76594126 14.39042819 17.23823934]\n"
     ]
    }
   ],
   "source": [
    "X_pred = np.array([50, 120, 175])\n",
    "# Use the same mean and standard deviation of the original training array X\n",
    "X_pred_norm = (X_pred - np.mean(X))/np.std(X)\n",
    "Y_pred_gd_norm = m_gd * X_pred_norm + b_gd\n",
    "# Use the same mean and standard deviation of the original training array Y\n",
    "Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)\n",
    "\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales using Gradient Descent:\\n{Y_pred_gd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo resultado que obtivemos com o SkLearn e com o Numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
